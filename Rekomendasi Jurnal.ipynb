{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sistem Rekomendasi Jurnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ASUS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing\n",
    "*Text preprocessing* atau prapemrosesan teks adalah serangkaian teknik yang digunakan untuk mempersiapkan dokumen teks untuk diproses oleh mesin pencari yang melibatkan beberapa tahap seperti:\n",
    "- Special characters removal\n",
    "- Stopwords removal\n",
    "- Casefolding\n",
    "- Tokenization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    # remove punctuation using string library\n",
    "    text = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    # remove certain characters\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Cleaning & Casefolding\n",
    "    text = remove_punctuation(text.lower())\n",
    "\n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berikut merupakan contoh hasil dari tahapan text preprocessing yang dilakukan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['exampl', 'text', 'preprocess', 'use', 'nltk', 'includ', 'casefold', 'token', 'stopword', 'remov', 'stem']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is an example of text preprocessing using NLTK. It includes casefolding, tokenization, stopword removal, and stemming.\"\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Ekstraksi fitur dilakukan dengan menerapkan tahap prapemrosesan kata pada data teks judul dan abstrak masing-masing dokumen yang terdapat dalam dataset, lalu menghitung vektor bobot TF-IDF untuk setiap data teks yang sudah terproses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil prapemrosesan teks:\n",
      "['dynam', 'studi', 'export', 'china', 'south', 'korea', 'econom', 'growth', 'china', 'paper', 'appli', 'annual', 'data', '1998', '2016', 'search', 'dynam', 'oper', 'mechan', 'export', 'china', 'south', 'korea', 'econom', 'growth', 'china', 'vector', 'error', 'correct', 'model', 'util', 'conduct', 'empir', 'analysi', 'result', 'indic', 'longrun', 'relationship', 'specif', 'export', 'china', 'south', 'korea', 'increas', '1', 'per', 'cent', 'econom', 'growth', 'china', 'increas', '0769', 'per', 'cent', 'meanwhil', 'result', 'granger', 'causal', 'test', 'also', 'reveal', 'unidirect', 'causal', 'exist', '5', 'per', 'cent', 'signific', 'level', 'importantli', 'result', 'vector', 'error', 'correct', 'mechan', 'show', 'econom', 'growth', 'deriv', 'longrun', 'equilibrium', 'short', 'run', 'return', 'longrun', 'equilibrium', '22', 'percent']\n",
      "['panel', 'approach', 'govern', 'expenditur', 'influenc', 'human', 'develop', 'index', 'studi', 'investig', 'influenc', 'govern', 'spend', 'educ', 'index', 'health', 'index', 'incom', 'index', 'region', 'underdevelop', 'develop', 'govern', 'east', 'java', 'indonesia', 'addit', 'paper', 'estim', 'influenc', 'govern', 'spend', 'develop', 'citi', 'district', 'east', 'java', 'studi', 'appli', 'quantit', 'approach', 'use', 'fix', 'effect', 'model', 'random', 'effect', 'model', 'panel', 'data', 'analysi', 'method', '38', 'citi', 'district', 'use', 'analysi', 'unit', '20102015', 'find', 'show', 'govern', 'spend', 'educ', 'health', 'econom', 'posit', 'signific', 'influenc', 'everi', 'compon', 'human', 'develop', 'index', 'addit', 'govern', 'spend', 'infrastructur', 'signific', 'influenc', 'educ', 'index', 'incom', 'index', 'yet', 'significantli', 'influenc', 'health', 'index', 'furthermor', 'studi', 'provid', 'differ', 'result', 'govern', 'spend', 'underdevelop', 'develop', 'region']\n",
      "['turnov', 'intent', 'public', 'account', 'firm', 'east', 'java', 'research', 'aim', 'investig', 'empir', 'influenc', 'pay', 'satisfact', 'role', 'conflict', 'role', 'ambigu', 'auditor', 'turnov', 'intent', 'mediat', 'job', 'satisfact', 'number', '168', 'auditor', 'work', 'public', 'account', 'firm', 'east', 'java', 'particip', 'research', 'data', 'analyz', 'use', 'pl', 'partial', 'least', 'squar', 'result', 'show', 'pay', 'satisfact', 'posit', 'influenc', 'job', 'satisfact', 'neg', 'influenc', 'auditor', '’', 'turnov', 'intent', 'role', 'conflict', 'direct', 'influenc', 'job', 'satisfact', 'direct', 'posit', 'influenc', 'toward', 'turnov', 'intent', 'role', 'ambigu', 'posit', 'influenc', 'job', 'satisfact', 'neg', 'influenc', 'auditor', '’', 'turnov', 'intent', 'besid', 'research', 'also', 'reveal', 'job', 'satisfact', 'partial', 'mediat', 'effect', 'pay', 'satisfact', 'auditor', '’', 'turnov', 'intent', 'full', 'mediat', 'effect', 'role', 'ambigu', 'auditor', '’', 'turnov', 'intent', 'mediat', 'effect', 'role', 'conflict', 'auditor', '’', 'turnov', 'intent']\n",
      "['factor', 'influenc', 'perceiv', 'eas', 'use', 'elearn', 'account', 'lectur', 'research', 'aim', 'analyz', 'influenc', 'one', '’', 'anxieti', 'use', 'comput', 'comput', 'anxieti', 'age', 'gender', 'perceiv', 'eas', 'use', 'elearn', 'account', 'lectur', 'selfefficaci', 'use', 'comput', 'comput', 'selfefficaci', 'popul', 'use', 'research', '34', 'lectur', 'account', 'depart', 'x', 'univers', 'sampl', 'taken', 'use', 'satur', 'sampl', 'techniqu', 'sinc', 'number', 'respond', 'limit', 'data', 'collect', 'distribut', 'questionnair', 'lectur', 'hypothes', 'test', 'use', 'path', 'analysi', 'test', 'research', 'result', 'indic', '1', 'comput', 'anxieti', 'insignific', 'influenc', 'perceiv', 'eas', 'use', 'comput', 'selfefficaci', '2', 'respond', '’', 'age', 'insignific', 'influenc', 'perceiv', 'eas', 'use', 'comput', 'selfefficaci', '3', 'respond', '’', 'gender', 'insignific', 'influenc', 'perceiv', 'eas', 'use', 'comput', 'selfefficaci']\n",
      "['rais', 'awar', 'primari', 'school', 'student', 'respect', 'biblioeduc', 'respect', 'contribut', 'restrain', 'frequent', 'occur', 'act', 'violenc', 'biblio', 'educ', 'belong', 'one', 'mean', 'establish', 'respect', 'studi', 'aim', 'investig', 'efficaci', 'biblio', 'educ', 'improv', 'elementari', 'student', 'awar', 'respect', 'quantit', 'studi', 'employ', 'equival', 'timeseri', 'design', 'ten', 'subject', 'state', 'elementari', 'school', 'gadingkasri', 'select', 'purpos', 'sampl', 'use', 'biblio', 'educ', 'treatment', 'eight', 'meet', 'valid', 'reliabl', 'respect', 'awar', 'scale', 'use', 'measur', 'instrument', 'data', 'analyz', 'use', 'nonparametr', 'data', 'analysi', 'wilcoxon', 'sign', 'rank', 'test', 'result', 'reveal', '14', 'treatment', 'carri', 'effect', 'student', 'awar', 'respect', 'impact', 'student', 'awar', 'respect', 'observ', 'treatment', '58', 'comprehens', 'biblio', 'educ', 'suffici', 'enhanc', 'elementari', 'school', 'student', 'awar', 'respect']\n",
      "\n",
      "Hasil perhitungan vektor bobot TF-IDF:\n",
      "[[0.         0.         0.07876521 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.16114261]\n",
      " [0.         0.         0.         ... 0.         0.         0.12283825]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "journalDtf = pd.read_csv(\"Datasets/data.csv\")\n",
    "journalDtf = journalDtf.drop('no', axis=1)\n",
    "journalDtf['combined'] = journalDtf['judul'] + \" \" + journalDtf['abstrak']\n",
    "\n",
    "features = []\n",
    "\n",
    "# Melakukan text preprocessing pada setiap teks dokumen\n",
    "for i in range(len(journalDtf)):\n",
    "    features.append(preprocess_text(journalDtf.loc[i, 'combined']))\n",
    "\n",
    "# Menghitung vector bobot TF-IDF masing-masing fitur \n",
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x)\n",
    "tfidfMatrix = vectorizer.fit_transform(features)\n",
    "\n",
    "# Print contoh hasil preprocessing kata\n",
    "print(\"Hasil prapemrosesan teks:\")\n",
    "\n",
    "for i in range(5):\n",
    "    print(features[i])\n",
    "\n",
    "# Print vektor tfidf dari dataset\n",
    "print(\"\\nHasil perhitungan vektor bobot TF-IDF:\")\n",
    "print(tfidfMatrix.toarray()[:5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pencarian Query and Perbandingan Cosine Similarity\n",
    "Pencarian dan perangkingan hasil pencarian query dilakukan dengan menghitung vektor bobot TF-IDF dan membandingkan seberapa dekat sudut vektor bobot query dengan vektor bobot lainnya yang ada pada dokumen dalam dataset. \n",
    "\n",
    "Perhitungan sudut ini dapat dilakukan dengan menghitung nilai $cos(\\theta)$ dari sudut antar vektor query dan vektor dokumen dengan formula berikut:\n",
    "\n",
    "$$cos(\\theta) = \\frac{\\vec{a} \\cdot \\vec{b}}{|\\vec{a}||\\vec{b}|}$$\n",
    "\n",
    "Hasil perhitungan kemudian akan digunakan sebagai acuan untuk mengurutkan hasil pencarian dokumen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSearchQueryRangking(query):\n",
    "    queryTfIdf = vectorizer.transform([preprocess_text(query)])\n",
    "    cosine_sim = cosine_similarity(queryTfIdf, tfidfMatrix)\n",
    "\n",
    "    return cosine_sim.argsort()[0][::-1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementasi UI sederhana untuk sistem pencarian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showSearchResults():\n",
    "    output_area.delete(\"1.0\", \"end\")\n",
    "    \n",
    "    query = input_field.get(\"1.0\", tk.END).strip()\n",
    "    rangkingIndex = getSearchQueryRangking(query)\n",
    "\n",
    "    for i in range(10):\n",
    "        output_area.insert(tk.END, journalDtf.loc[rangkingIndex[i], 'judul'] + \"\\n\\n\")\n",
    "\n",
    "window = tk.Tk()\n",
    "\n",
    "input_field = tk.Text(window, height=1)\n",
    "input_field.pack()\n",
    "\n",
    "search_button = tk.Button(window, text=\"Search\", command=showSearchResults)\n",
    "search_button.pack()\n",
    "\n",
    "output_area = tk.Text(window)\n",
    "output_area.pack()\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAP      :  3.1686507936507935\n",
      "Precision:  0.45454545454545453\n",
      "Recall   :  0.7142857142857143\n",
      "[[6, 14], [1, 0]]\n"
     ]
    }
   ],
   "source": [
    "query = \"Education\"\n",
    "relevantIndexes = [4, 5, 11, 12, 18, 19, 20]\n",
    "\n",
    "rangkingIndex = getSearchQueryRangking(query)\n",
    "searchResults = 10\n",
    "\n",
    "confMatrix = [[0, 0], [0, 0]]\n",
    "precision = 0\n",
    "recall = 0\n",
    "MAPval = 0\n",
    "\n",
    "\n",
    "for i in  range(len(rangkingIndex)):\n",
    "    if rangkingIndex[i] in relevantIndexes:\n",
    "        confMatrix[0][0] += 1\n",
    "        MAPval += confMatrix[0][0]/(confMatrix[0][0] + confMatrix[0][1])\n",
    "    else:\n",
    "        confMatrix[0][1] += 1\n",
    "    \n",
    "    confMatrix[1][0] = len(relevantIndexes) - confMatrix[0][0]\n",
    "    confMatrix[1][1] = len(rangkingIndex) - i - 1\n",
    "\n",
    "    if i == searchResults:\n",
    "        precision = confMatrix[0][0] / (confMatrix[0][0] + confMatrix[0][1])\n",
    "        recall = confMatrix[0][0] / (confMatrix[0][0] + confMatrix[1][0])\n",
    "\n",
    "MAPval / confMatrix[0][0]\n",
    "\n",
    "print(\"MAP      : \", MAPval)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall   : \", recall)\n",
    "print(confMatrix)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTubesIR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
